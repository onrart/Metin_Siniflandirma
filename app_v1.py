# app.py
import os
from typing import List, Dict, Any, Optional, Tuple

import io
import pandas as pd
import streamlit as st
import torch
from transformers import (
    pipeline,
    AutoConfig,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from huggingface_hub import snapshot_download

# .env (opsiyonel)
try:
    from dotenv import load_dotenv

    load_dotenv()
except Exception:
    pass

st.set_page_config(page_title="Metin SÄ±nÄ±flandÄ±rma (HF Pipeline)", layout="wide")

# ==========================
# Ortam deÄŸiÅŸkenleri (varsayÄ±lanlar)
# ==========================
ENV_MODEL_PATH = os.getenv("MODEL_PATH", "onrart/final_model")  # default
ENV_MODEL_REVISION = os.getenv("MODEL_REVISION", None)          # Ã¶rn: "main" veya commit hash

def _parse_int(s: Optional[str]) -> Optional[int]:
    try:
        return int(s) if s is not None and s != "" else None
    except Exception:
        return None

ENV_DEVICE_INDEX = _parse_int(os.getenv("DEVICE_INDEX", None))  # -1=CPU, None=Auto(GPU0), 0..N=GPU

# ==========================
# YardÄ±mcÄ±lar
# ==========================
def available_devices() -> Tuple[List[str], List[Optional[int]]]:
    """UI iÃ§in gÃ¶rÃ¼nen isimler ve karÅŸÄ±lÄ±k gelen index/sentinel listesi dÃ¶ndÃ¼rÃ¼r."""
    if torch.cuda.is_available():
        count = torch.cuda.device_count()
        names = ["Otomatik (CUDA varsa GPU)", "CPU"] + [
            f"GPU:{i} - {torch.cuda.get_device_name(i)}" for i in range(count)
        ]
        idxs: List[Optional[int]] = [None, -1] + list(range(count))
    else:
        names = ["CPU"]
        idxs = [-1]
    return names, idxs

def resolve_device(device_index: Optional[int]) -> int:
    """
    GeÃ§erli bir pipeline cihaz indexi dÃ¶ndÃ¼rÃ¼r:
    - CUDA varsa ve device_index None (Auto) -> 0
    - device_index 0..cuda_count-1 ise -> kendisi
    - aksi halde -> -1 (CPU)
    """
    if torch.cuda.is_available():
        cuda_count = torch.cuda.device_count()
        if device_index is None:  # Auto
            return 0
        if isinstance(device_index, int) and 0 <= device_index < cuda_count:
            return device_index
    return -1  # CPU fallback

def _is_local_path(path: str) -> bool:
    return os.path.isdir(path)

def _resolve_model_source(model_path: str, revision: Optional[str]) -> str:
    """
    HF repo id veya yerel klasÃ¶r olabilir.
    - Yerel klasÃ¶rse doÄŸrudan onu dÃ¶ndÃ¼rÃ¼r.
    - Repo id ise:
        - revision varsa snapshot indir ve yerel yolu dÃ¶ndÃ¼r (sabit sÃ¼rÃ¼m).
        - yoksa repo id'yi dÃ¶ndÃ¼r (transformers otomatik indir/cache'ler).
    """
    if _is_local_path(model_path):
        return model_path
    if revision:
        local_path = snapshot_download(repo_id=model_path, revision=revision)
        return local_path
    return model_path

@st.cache_resource(show_spinner=False)
def load_classifier(model_id_or_path: str, device_index: Optional[int], revision: Optional[str]):
    """
    Tokenizer + Model ayrÄ± yÃ¼klenir (torch_dtype='auto') -> pipeline dÃ¶ndÃ¼rÃ¼r.
    AyrÄ±ca (labels, device_str) dÃ¶ner.
    Cache: model_id_or_path, device_index, revision Ã¼Ã§lÃ¼sÃ¼ne gÃ¶re.
    """
    device_for_pipeline = resolve_device(device_index)
    model_to_load = _resolve_model_source(model_id_or_path, revision)

    # Tokenizer + Model
    tok = AutoTokenizer.from_pretrained(model_to_load)
    mdl = AutoModelForSequenceClassification.from_pretrained(
        model_to_load,
        torch_dtype="auto",  # CUDA'da fp16/bf16; CPU'da fp32
    )
    mdl.eval()

    clf = pipeline(
        task="text-classification",
        model=mdl,
        tokenizer=tok,
        device=device_for_pipeline,
    )

    # Etiketleri config'ten Ã§ek (varsa)
    labels: List[str] = []
    try:
        cfg = AutoConfig.from_pretrained(model_to_load)
        id2label = getattr(cfg, "id2label", None)
        if isinstance(id2label, dict) and len(id2label) > 0:
            labels = [id2label[i] for i in sorted(map(int, id2label.keys()))]
    except Exception:
        pass

    # Hafif Ä±sÄ±nma (sadece GPU'da)
    try:
        if device_for_pipeline != -1:
            _ = clf(["warmup"], top_k=1, truncation=True, max_length=32)
    except Exception:
        pass

    device_str = "cuda" if device_for_pipeline != -1 else "cpu"
    return clf, labels, device_str

def predict_texts(
    clf,
    texts: List[str],
    multi_label: bool,
    threshold: float,
    top_k: int,
    max_length: int,
    truncation: bool,
    pipeline_batch_size: int = 32,
) -> List[Dict[str, Any]]:
    """Metin listesi iÃ§in tahminleri dÃ¶ndÃ¼rÃ¼r."""
    if not texts:
        return []

    threshold = max(0.0, min(1.0, float(threshold)))
    top_k = max(1, int(top_k))
    max_length = max(8, int(max_length))
    pipeline_batch_size = max(1, int(pipeline_batch_size))

    tokenizer_kwargs = {
        "truncation": truncation,
        "max_length": max_length,
    }
    apply_fn = "sigmoid" if multi_label else "softmax"

    results: List[Dict[str, Any]] = []

    if multi_label:
        raw = clf(
            texts,
            return_all_scores=True,
            batch_size=pipeline_batch_size,
            function_to_apply=apply_fn,
            **tokenizer_kwargs,
        )
        for text, scores in zip(texts, raw):
            filtered = [s for s in scores if float(s["score"]) >= threshold]
            if not filtered:
                filtered = [max(scores, key=lambda s: float(s["score"]))]  # en yÃ¼ksek en az 1
            filtered.sort(key=lambda s: float(s["score"]), reverse=True)
            results.append(
                {
                    "text": text,
                    "predictions": [
                        {"label": s["label"], "score": round(float(s["score"]), 6)}
                        for s in filtered
                    ],
                }
            )
    else:
        raw = clf(
            texts,
            top_k=top_k,
            batch_size=pipeline_batch_size,
            function_to_apply=apply_fn,
            **tokenizer_kwargs,
        )
        for text, out in zip(texts, raw):
            preds = out if isinstance(out, list) else [out]
            preds.sort(key=lambda s: float(s["score"]), reverse=True)
            results.append(
                {
                    "text": text,
                    "predictions": [
                        {"label": p["label"], "score": round(float(p["score"]), 6)}
                        for p in preds
                    ],
                }
            )
    return results

def format_results_dataframe(results: List[Dict[str, Any]], multi_label: bool) -> pd.DataFrame:
    if not results:
        return pd.DataFrame()
    rows = []
    for r in results:
        text = r["text"]
        preds = r["predictions"]
        row = {
            "text": text,
            "predicted_labels": ", ".join([p["label"] for p in preds]),
            "predicted_scores": ", ".join([str(p["score"]) for p in preds]),
        }
        if not multi_label and preds:
            row["best_label"] = preds[0]["label"]
            row["best_score"] = preds[0]["score"]
        rows.append(row)
    return pd.DataFrame(rows)

def format_results_wide(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """Her etiket iÃ§in ayrÄ± skor kolonu (yoksa 0.0) ekler."""
    if not results:
        return pd.DataFrame()
    all_labels = sorted({p["label"] for r in results for p in r["predictions"]})
    rows = []
    for r in results:
        row = {"text": r["text"]}
        for lbl in all_labels:
            row[f"score__{lbl}"] = 0.0
        for p in r["predictions"]:
            row[f"score__{p['label']}"] = float(p["score"])
        rows.append(row)
    return pd.DataFrame(rows)

def robust_read_csv(uploaded_file) -> pd.DataFrame:
    """Encoding/ayraÃ§ esnek CSV okuyucu."""
    raw = uploaded_file.read()
    # Basit bir yaklaÅŸÄ±m: birkaÃ§ ayraÃ§ dener
    for sep in [",", ";", "\t", "|"]:
        try:
            return pd.read_csv(io.BytesIO(raw), sep=sep)
        except Exception:
            continue
    return pd.read_csv(io.BytesIO(raw), engine="python")

# ==========================
# UI â€” Sidebar: Model SeÃ§imi ve YÃ¼kleme
# ==========================
st.sidebar.header("âš™ï¸ Ayarlar")

# HazÄ±r modeller
PREDEFINED_MODELS = {
    "Final Model": "onrart/final_model",
    "BERTurk v1": "onrart/bertuk_v1",
    "mDeBERTa v1": "onrart/mdeberta_v1",
    "Custom (kendi yolum)": None,
}

# Cihaz seÃ§imi: ENV_DEVICE_INDEX'e gÃ¶re baÅŸlangÄ±Ã§ seÃ§imi
device_names, device_idxs = available_devices()

def _default_device_index_from_env() -> int:
    if torch.cuda.is_available():
        if ENV_DEVICE_INDEX is None:
            return 0  # Auto
        if ENV_DEVICE_INDEX == -1:
            return 1  # CPU
        try:
            return device_idxs.index(ENV_DEVICE_INDEX)
        except ValueError:
            return 0
    else:
        return 0

default_idx = _default_device_index_from_env()
device_choice = st.sidebar.selectbox("Cihaz", device_names, index=default_idx)
chosen_device_index: Optional[int] = device_idxs[device_names.index(device_choice)]

# Revision (opsiyonel)
revision = st.sidebar.text_input(
    "MODEL_REVISION (opsiyonel)",
    value=ENV_MODEL_REVISION or "",
    help="Ã–rn: 'main' veya bir commit hash; boÅŸ bÄ±rakÄ±lÄ±rsa en son sÃ¼rÃ¼m kullanÄ±lÄ±r",
)
revision = revision if revision.strip() else None

# Pipeline batch size
pipeline_batch_size = st.sidebar.number_input(
    "Pipeline batch size", min_value=1, max_value=128, value=32, step=1
)

# Tekil â€œaktif modelâ€ seÃ§imi iÃ§in menÃ¼ (predefined + custom)
model_choice = st.sidebar.selectbox(
    "Model seÃ§ (aktif)",
    options=list(PREDEFINED_MODELS.keys()),
    index=0,
)

if PREDEFINED_MODELS[model_choice] is None:
    # KullanÄ±cÄ± Ã¶zel model girecek
    custom_model_path = st.sidebar.text_input(
        "Custom model yolu (HF repo id veya yerel klasÃ¶r)",
        value=ENV_MODEL_PATH,
        help="Ã–rn: onrart/final_model veya ./final_model",
    )
    active_model_path = custom_model_path.strip()
else:
    active_model_path = PREDEFINED_MODELS[model_choice]

# Ã‡oklu model Ã¶n-yÃ¼kleme (baÅŸlangÄ±Ã§ta)
st.sidebar.markdown("---")
st.sidebar.subheader("ðŸ§° BaÅŸlangÄ±Ã§ta Ã¶n-yÃ¼klenecek modeller")
preload_options = [k for k in PREDEFINED_MODELS.keys() if PREDEFINED_MODELS[k] is not None]
# EÄŸer custom girilmiÅŸse onu da seÃ§enek olarak ekle
if PREDEFINED_MODELS[model_choice] is None and active_model_path:
    preload_options = preload_options + ["Custom (ÅŸu an girilen)"]

preload_selection = st.sidebar.multiselect(
    "Modelleri seÃ§ (isteÄŸe baÄŸlÄ±)",
    options=preload_options,
    default=["Final Model"],  # varsayÄ±lan Ã¶n-yÃ¼kleme
)

if "loaded_models" not in st.session_state:
    # loaded_models: Dict[gÃ¶rÃ¼nen_ad, dict(path=..., clf=..., labels=..., device=...)]
    st.session_state.loaded_models = {}

def _add_loaded_model(display_name: str, path: str):
    if display_name in st.session_state.loaded_models:
        return
    clf, labels, device_used = load_classifier(path, chosen_device_index, revision)
    st.session_state.loaded_models[display_name] = {
        "path": path,
        "clf": clf,
        "labels": labels,
        "device": device_used,
    }

# Ã–n-yÃ¼kleme butonu
if st.sidebar.button("SeÃ§ili modelleri Ã¶n-yÃ¼kle", type="primary", use_container_width=True):
    try:
        for name in preload_selection:
            if name == "Custom (ÅŸu an girilen)":
                if active_model_path:
                    _add_loaded_model("Custom", active_model_path)
            else:
                _add_loaded_model(name, PREDEFINED_MODELS[name])
        st.sidebar.success("Ã–n-yÃ¼kleme tamam.")
    except Exception as e:
        st.sidebar.error(f"Ã–n-yÃ¼kleme hatasÄ±: {e}")

# Aktif modeli yÃ¼kle (eÄŸer yÃ¼klÃ¼ deÄŸilse tek baÅŸÄ±na da yÃ¼klenebilir)
if st.sidebar.button("Aktif modeli yÃ¼kle", use_container_width=True):
    try:
        disp_name = model_choice if PREDEFINED_MODELS[model_choice] is not None else "Custom"
        _add_loaded_model(disp_name, active_model_path)
        st.sidebar.success(f"Aktif model yÃ¼klendi: {disp_name}")
    except Exception as e:
        st.sidebar.error(f"Model yÃ¼klenemedi: {e}")

# YÃ¼klÃ¼ modeller listesi ve aktif seÃ§im
loaded_names = list(st.session_state.loaded_models.keys())
if loaded_names:
    st.sidebar.markdown("---")
    st.sidebar.subheader("âœ… YÃ¼klÃ¼ modeller")
    pick_active_loaded = st.sidebar.selectbox(
        "Aktif (yÃ¼klÃ¼) modeli seÃ§",
        options=loaded_names,
        index=loaded_names.index(loaded_names[0]),
    )
    active_loaded = st.session_state.loaded_models[pick_active_loaded]
    classifier = active_loaded["clf"]
    labels = active_loaded["labels"]
    device_used = active_loaded["device"]
    st.sidebar.success(f"Aktif (yÃ¼klÃ¼) model: {pick_active_loaded} Â· Cihaz: {device_used}")
    if revision:
        st.sidebar.caption(f"ðŸ“Œ KullanÄ±lan sÃ¼rÃ¼m: {revision}")
    if labels:
        st.sidebar.caption("Etiketler:")
        st.sidebar.write(labels)
else:
    # HiÃ§bir model Ã¶nceden yÃ¼klenmediyse, anlÄ±k yÃ¼kle ve kullan
    with st.sidebar:
        try:
            with st.spinner("Model yÃ¼kleniyor..."):
                classifier, labels, device_used = load_classifier(active_model_path, chosen_device_index, revision)
            st.success(f"Model yÃ¼klendi Â· Cihaz: {device_used}")
            if revision:
                st.caption(f"ðŸ“Œ KullanÄ±lan sÃ¼rÃ¼m: {revision}")
            if labels:
                st.caption("Etiketler:")
                st.write(labels)
        except Exception as e:
            st.error(f"Model yÃ¼klenemedi: {e}")
            st.stop()

# ==========================
# Genel sÄ±nÄ±flandÄ±rma ayarlarÄ±
# ==========================
multi_label = st.sidebar.toggle("Multi-label", value=False, help="Birden fazla etiket aynÄ± anda seÃ§ilebilir")
threshold = st.sidebar.slider("EÅŸik (multi-label)", 0.0, 1.0, 0.5, 0.01, disabled=not multi_label)
top_k = st.sidebar.number_input(
    "Top-K (single-label)", min_value=1, max_value=10, value=3, step=1, disabled=multi_label
)
max_length = st.sidebar.number_input("max_length", min_value=8, max_value=4096, value=512, step=8)
truncation = st.sidebar.toggle("Truncation", value=True)
csv_batch_size = st.sidebar.number_input("CSV batch size", min_value=8, max_value=4096, value=256, step=8)

# ==========================
# UI â€” Ä°Ã§erik
# ==========================
st.title("ðŸ§  Metin SÄ±nÄ±flandÄ±rma â€” Streamlit (HF Pipeline)")

tabs = st.tabs(["Tek Metin", "Ã‡oklu Metin", "CSV YÃ¼kle"])

# ---- Tek Metin
with tabs[0]:
    st.subheader("Tek Metin Tahmini")
    text = st.text_area(
        "Metin",
        placeholder="Ã–rn: kargo geÃ§ geldi ama mÃ¼ÅŸteri hizmetleri iyiydi",
        height=140,
    )
    if st.button("Tahmin Et", type="primary", use_container_width=True, disabled=not text.strip()):
        with st.spinner("Tahmin yapÄ±lÄ±yor..."):
            res = predict_texts(
                classifier,
                [text],
                multi_label=multi_label,
                threshold=threshold,
                top_k=top_k,
                max_length=max_length,
                truncation=truncation,
                pipeline_batch_size=pipeline_batch_size,
            )
        df = format_results_dataframe(res, multi_label=multi_label)
        st.dataframe(df, use_container_width=True)
        st.caption("Skorlar 0â€“1 aralÄ±ÄŸÄ±ndadÄ±r.")
        if not multi_label and res and len(res[0]["predictions"]) > 1:
            chart_df = pd.DataFrame(res[0]["predictions"])
            st.bar_chart(chart_df.set_index("label")["score"], use_container_width=True)

# ---- Ã‡oklu Metin
with tabs[1]:
    st.subheader("Ã‡oklu Metin (her satÄ±r 1 metin)")
    bulk_texts = st.text_area(
        "Metinler",
        placeholder="Bir satÄ±ra bir metin olacak ÅŸekilde yapÄ±ÅŸtÄ±r.",
        height=220,
    )
    add_wide = st.checkbox("Wide format skor kolonlarÄ±nÄ± da ekle (score__etiket)", value=True)
    if st.button(
        "Toplu Tahmin Et",
        type="primary",
        use_container_width=True,
        disabled=not bulk_texts.strip(),
    ):
        texts = [t.strip() for t in bulk_texts.splitlines() if t.strip()]
        with st.spinner(f"{len(texts)} metin iÃ§in tahmin yapÄ±lÄ±yor..."):
            res = predict_texts(
                classifier,
                texts,
                multi_label=multi_label,
                threshold=threshold,
                top_k=top_k,
                max_length=max_length,
                truncation=truncation,
                pipeline_batch_size=pipeline_batch_size,
            )
        df_base = format_results_dataframe(res, multi_label=multi_label)
        if add_wide:
            df_wide = format_results_wide(res)
            df = df_base.merge(df_wide, on="text", how="left")
        else:
            df = df_base
        st.dataframe(df, use_container_width=True)
        csv_bytes = df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "SonuÃ§larÄ± CSV indir",
            data=csv_bytes,
            file_name="predictions.csv",
            mime="text/csv",
        )

# ---- CSV YÃ¼kle
with tabs[2]:
    st.subheader("CSV YÃ¼kle ve Tahmin")
    up = st.file_uploader("CSV seÃ§ (utf-8, ; , \\t | desteklenir)", type=["csv"])
    text_col = st.text_input("Metin sÃ¼tunu adÄ±", value="text")
    add_wide_csv = st.checkbox("Wide format skor kolonlarÄ±nÄ± ekle (score__etiket)", value=True)
    if up is not None:
        try:
            df_in = robust_read_csv(up)
        except Exception as e:
            st.error(f"CSV okunamadÄ±: {e}")
            st.stop()
        st.write("Ã–rnek ilk 5 satÄ±r:")
        st.dataframe(df_in.head(), use_container_width=True)
        if text_col not in df_in.columns:
            st.error(f"'{text_col}' sÃ¼tunu bulunamadÄ±.")
        else:
            if st.button("CSV'yi Tahmin Et", type="primary", use_container_width=True):
                texts = df_in[text_col].astype(str).tolist()
                results_all: List[Dict[str, Any]] = []
                with st.spinner(f"{len(texts)} satÄ±r iÅŸleniyor..."):
                    total = len(texts)
                    done = 0
                    prog = st.progress(0.0)
                    for i in range(0, total, int(csv_batch_size)):
                        chunk = texts[i : i + int(csv_batch_size)]
                        res = predict_texts(
                            classifier,
                            chunk,
                            multi_label=multi_label,
                            threshold=threshold,
                            top_k=top_k,
                            max_length=max_length,
                            truncation=truncation,
                            pipeline_batch_size=pipeline_batch_size,
                        )
                        results_all.extend(res)
                        done += len(chunk)
                        prog.progress(min(1.0, done / max(1, total)))
                df_out = format_results_dataframe(results_all, multi_label=multi_label)
                if add_wide_csv:
                    df_wide = format_results_wide(results_all)
                    df_final = pd.concat(
                        [
                            df_in.reset_index(drop=True),
                            df_out.drop(columns=["text"], errors="ignore").reset_index(drop=True),
                            df_wide.drop(columns=["text"], errors="ignore").reset_index(drop=True),
                        ],
                        axis=1,
                    )
                else:
                    df_final = pd.concat(
                        [
                            df_in.reset_index(drop=True),
                            df_out.drop(columns=["text"], errors="ignore").reset_index(drop=True),
                        ],
                        axis=1,
                    )
                st.success("TamamlandÄ±.")
                st.dataframe(df_final.head(50), use_container_width=True)
                csv = df_final.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "SonuÃ§larÄ± CSV indir",
                    data=csv,
                    file_name="predictions_with_input.csv",
                    mime="text/csv",
                )

st.caption(
    "Notlar: 1) GPU kullanÄ±yorsan doÄŸru CUDA Torch paketini kur. "
    "2) Private HF repo iÃ§in HUGGINGFACE_HUB_TOKEN ortam deÄŸiÅŸkenini ayarla. "
    "3) MODEL_REVISION vererek belirli bir commit'e sabitleyebilirsin. "
    "4) Ã‡oklu model Ã¶n-yÃ¼kleme iÃ§in sol menÃ¼den seÃ§im yap; 'Aktif (yÃ¼klÃ¼) modeli seÃ§' ile hÄ±zla geÃ§iÅŸ yap."
)
